{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "Ready to build your Stock Screener!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# This cell contains the necessary import statements for the required libraries and modules.\n",
    "# It also includes any initial setup or configuration needed for the program to run correctly.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"Ready to build your Stock Screener!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 We'll analyze 50 stocks:\n",
      "['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK-B', 'UNH', 'JNJ'] ... and more\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Define Stock Tickers\n",
    "# ============================================================================\n",
    "\n",
    "# List of popular stock tickers \n",
    "STOCK_TICKERS = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'BRK-B', 'UNH', 'JNJ',\n",
    "    'V', 'WMT', 'JPM', 'MA', 'PG', 'CVX', 'HD', 'PFE', 'BAC', 'ABBV',\n",
    "    'KO', 'AVGO', 'PEP', 'TMO', 'COST', 'MRK', 'ACN', 'DHR', 'VZ', 'ABT',\n",
    "    'ADBE', 'LLY', 'CRM', 'TXN', 'NKE', 'NFLX', 'AMD', 'QCOM', 'NEE', 'BMY',\n",
    "    'UPS', 'PM', 'T', 'LOW', 'ORCL', 'MDT', 'UNP', 'IBM', 'CAT', 'GS'\n",
    "]\n",
    "\n",
    "print(f\"📊 We'll analyze {len(STOCK_TICKERS)} stocks:\")\n",
    "print(STOCK_TICKERS[:10], \"... and more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data collection functions defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Define Data Collection Functions\n",
    "# ============================================================================\n",
    "\n",
    "def get_stock_data(ticker):\n",
    "    \"\"\"Get comprehensive stock data for analysis\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get basic info\n",
    "        info = stock.info\n",
    "        \n",
    "        # Get historical data for calculations\n",
    "        hist = stock.history(period=\"1y\")\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Calculate metrics\n",
    "        current_price = hist['Close'].iloc[-1]\n",
    "        year_high = hist['High'].max()\n",
    "        year_low = hist['Low'].min()\n",
    "        \n",
    "        # Calculate returns and volatility\n",
    "        returns = hist['Close'].pct_change().dropna()\n",
    "        annual_return = ((current_price / hist['Close'].iloc[0]) - 1) * 100\n",
    "        volatility = returns.std() * np.sqrt(252) * 100  # Annualized volatility\n",
    "        \n",
    "        # Extract key metrics\n",
    "        data = {\n",
    "            'ticker': ticker,\n",
    "            'company_name': info.get('longName', ticker),\n",
    "            'current_price': current_price,\n",
    "            'market_cap': info.get('marketCap', 0),\n",
    "            'pe_ratio': info.get('trailingPE', None),\n",
    "            'forward_pe': info.get('forwardPE', None),\n",
    "            'peg_ratio': info.get('pegRatio', None),\n",
    "            'debt_to_equity': info.get('debtToEquity', None),\n",
    "            'roe': info.get('returnOnEquity', None),\n",
    "            'profit_margin': info.get('profitMargins', None),\n",
    "            'revenue_growth': info.get('revenueGrowth', None),\n",
    "            'annual_return': annual_return,\n",
    "            'volatility': volatility,\n",
    "            'year_high': year_high,\n",
    "            'year_low': year_low,\n",
    "            'price_to_high_ratio': (current_price / year_high) * 100,\n",
    "            'dividend_yield': info.get('dividendYield', 0) * 100 if info.get('dividendYield') else 0,\n",
    "            'beta': info.get('beta', None),\n",
    "            'sector': info.get('sector', 'Unknown')\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Data collection functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Collecting stock data... This may take a few minutes.\n",
      "Grab a coffee ☕ while we fetch real-time financial data!\n",
      "Processing AAPL (1/50) ✅\n",
      "Processing MSFT (2/50) ✅\n",
      "Processing GOOGL (3/50) ✅\n",
      "Processing AMZN (4/50) ✅\n",
      "Processing TSLA (5/50) ✅\n",
      "Processing META (6/50) ✅\n",
      "Processing NVDA (7/50) ✅\n",
      "Processing BRK-B (8/50) ✅\n",
      "Processing UNH (9/50) ✅\n",
      "Processing JNJ (10/50) ✅\n",
      "Processing V (11/50) ✅\n",
      "Processing WMT (12/50) ✅\n",
      "Processing JPM (13/50) ✅\n",
      "Processing MA (14/50) ✅\n",
      "Processing PG (15/50) ✅\n",
      "Processing CVX (16/50) ✅\n",
      "Processing HD (17/50) ✅\n",
      "Processing PFE (18/50) ✅\n",
      "Processing BAC (19/50) ✅\n",
      "Processing ABBV (20/50) ✅\n",
      "Processing KO (21/50) ✅\n",
      "Processing AVGO (22/50) ✅\n",
      "Processing PEP (23/50) ✅\n",
      "Processing TMO (24/50) ✅\n",
      "Processing COST (25/50) ✅\n",
      "Processing MRK (26/50) ✅\n",
      "Processing ACN (27/50) ✅\n",
      "Processing DHR (28/50) ✅\n",
      "Processing VZ (29/50) ✅\n",
      "Processing ABT (30/50) ✅\n",
      "Processing ADBE (31/50) ✅\n",
      "Processing LLY (32/50) ✅\n",
      "Processing CRM (33/50) ✅\n",
      "Processing TXN (34/50) ✅\n",
      "Processing NKE (35/50) ✅\n",
      "Processing NFLX (36/50) ✅\n",
      "Processing AMD (37/50) ✅\n",
      "Processing QCOM (38/50) ✅\n",
      "Processing NEE (39/50) ✅\n",
      "Processing BMY (40/50) ✅\n",
      "Processing UPS (41/50) ✅\n",
      "Processing PM (42/50) ✅\n",
      "Processing T (43/50) ✅\n",
      "Processing LOW (44/50) ✅\n",
      "Processing ORCL (45/50) ✅\n",
      "Processing MDT (46/50) ✅\n",
      "Processing UNP (47/50) ✅\n",
      "Processing IBM (48/50) ✅\n",
      "Processing CAT (49/50) ✅\n",
      "Processing GS (50/50) ✅\n",
      "\n",
      "🎉 Successfully collected data for 50 stocks!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Collect Stock Data (This will take 3-5 minutes)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🔄 Collecting stock data... This may take a few minutes.\")\n",
    "print(\"Grab a coffee ☕ while we fetch real-time financial data!\")\n",
    "\n",
    "stock_data = []\n",
    "for i, ticker in enumerate(STOCK_TICKERS):\n",
    "    print(f\"Processing {ticker} ({i+1}/{len(STOCK_TICKERS)})\", end=\" \")\n",
    "    data = get_stock_data(ticker)\n",
    "    if data:\n",
    "        stock_data.append(data)\n",
    "        print(\"✅\")\n",
    "    else:\n",
    "        print(\"❌\")\n",
    "\n",
    "print(f\"\\n🎉 Successfully collected data for {len(stock_data)} stocks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Raw data shape: (50, 19)\n",
      "🧹 Cleaned data shape: (47, 19)\n",
      "✨ Final dataset has 47 stocks ready for analysis!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Create and Clean DataFrame\n",
    "# ============================================================================\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(stock_data)\n",
    "\n",
    "print(\"📊 Raw data shape:\", df.shape)\n",
    "\n",
    "# Clean the data\n",
    "df_clean = df.dropna(subset=['pe_ratio', 'market_cap'])\n",
    "df_clean = df_clean[df_clean['market_cap'] > 0]\n",
    "df_clean = df_clean[df_clean['pe_ratio'] > 0]\n",
    "df_clean = df_clean[df_clean['pe_ratio'] < 100]  # Remove extreme PE ratios\n",
    "\n",
    "print(\"🧹 Cleaned data shape:\", df_clean.shape)\n",
    "print(f\"✨ Final dataset has {len(df_clean)} stocks ready for analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 STOCK SCREENER DATA PREVIEW\n",
      "==================================================\n",
      "   ticker                     company_name  current_price   pe_ratio  \\\n",
      "0    AAPL                       Apple Inc.     232.139999  35.226100   \n",
      "1    MSFT            Microsoft Corporation     506.690002  37.174614   \n",
      "2   GOOGL                    Alphabet Inc.     212.910004  22.674122   \n",
      "3    AMZN                 Amazon.com, Inc.     229.000000  34.961830   \n",
      "5    META             Meta Platforms, Inc.     738.700012  26.813068   \n",
      "6    NVDA               NVIDIA Corporation     174.179993  49.463352   \n",
      "7   BRK-B          Berkshire Hathaway Inc.     502.980011  17.243060   \n",
      "8     UNH  UnitedHealth Group Incorporated     309.869995  13.420095   \n",
      "9     JNJ                Johnson & Johnson     177.169998  18.968950   \n",
      "10      V                        Visa Inc.     351.779999  34.353516   \n",
      "\n",
      "       market_cap  annual_return  \n",
      "0   3445050310656       1.842964  \n",
      "1   3766312763392      22.379535  \n",
      "2   2578297651200      30.947089  \n",
      "3   2442261954560      28.291317  \n",
      "5   1855717900288      42.185917  \n",
      "6   4239080554496      45.962144  \n",
      "7   1085516414976       5.685829  \n",
      "8    280641208320     -46.506097  \n",
      "9    426685595648      10.272010  \n",
      "10   682783866880      28.179767  \n",
      "\n",
      "📊 DATASET STATISTICS\n",
      "==============================\n",
      "Total Stocks: 47\n",
      "Average PE Ratio: 28.98\n",
      "Average Annual Return: 9.60%\n",
      "Sectors Covered: 9\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Explore the Data\n",
    "# ============================================================================\n",
    "\n",
    "# Display basic info\n",
    "print(\"📈 STOCK SCREENER DATA PREVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show sample data\n",
    "display_cols = ['ticker', 'company_name', 'current_price', 'pe_ratio', 'market_cap', 'annual_return']\n",
    "print(df_clean[display_cols].head(10))\n",
    "\n",
    "print(\"\\n📊 DATASET STATISTICS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total Stocks: {len(df_clean)}\")\n",
    "print(f\"Average PE Ratio: {df_clean['pe_ratio'].mean():.2f}\")\n",
    "print(f\"Average Annual Return: {df_clean['annual_return'].mean():.2f}%\")\n",
    "print(f\"Sectors Covered: {df_clean['sector'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Data saved to 'stock_data.csv'\n",
      "🚀 Ready for Phase 2 - Building the ML Model!\n",
      "✅ Data stored in 'stock_df' variable for ML modeling\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Save Data\n",
    "# ============================================================================\n",
    "\n",
    "# Save to CSV for later use\n",
    "df_clean.to_csv('stock_data.csv', index=False)\n",
    "print(\"💾 Data saved to 'stock_data.csv'\")\n",
    "print(\"🚀 Ready for Phase 2 - Building the ML Model!\")\n",
    "\n",
    "# Make df_clean available for next phases\n",
    "stock_df = df_clean.copy()\n",
    "print(\"✅ Data stored in 'stock_df' variable for ML modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ML libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Import ML Libraries\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ ML libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Starting with 47 stocks\n",
      "🧹 After cleaning: 47 stocks with valid scores\n",
      "📊 Final dataset: 47 stocks with 10 features\n",
      "Features: ['pe_ratio', 'market_cap', 'annual_return', 'volatility', 'price_to_high_ratio', 'dividend_yield', 'debt_to_equity', 'roe', 'profit_margin', 'beta']\n",
      "🎯 Target variable: investment_score (range: 0.141 - 0.741)\n",
      "\n",
      "✅ Feature engineering complete!\n",
      "Features shape: (47, 10)\n",
      "Target shape: (47,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Feature Engineering & Target Creation (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "def create_features_and_target(df):\n",
    "    \"\"\"Create ML features and target variable\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    ml_df = df.copy()\n",
    "    \n",
    "    print(f\"🔧 Starting with {len(ml_df)} stocks\")\n",
    "    \n",
    "    # Fill missing values with median for numeric columns\n",
    "    numeric_cols = ['pe_ratio', 'debt_to_equity', 'roe', 'profit_margin', \n",
    "                   'revenue_growth', 'beta', 'peg_ratio', 'forward_pe']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in ml_df.columns:\n",
    "            ml_df[col] = ml_df[col].fillna(ml_df[col].median())\n",
    "    \n",
    "    # Ensure key columns exist and are not all NaN\n",
    "    required_cols = ['annual_return', 'volatility', 'pe_ratio']\n",
    "    for col in required_cols:\n",
    "        if col not in ml_df.columns or ml_df[col].isna().all():\n",
    "            print(f\"❌ Missing required column: {col}\")\n",
    "            return None, None, None, None\n",
    "    \n",
    "    # Create composite target score (what we want to predict/rank)\n",
    "    # Higher score = better investment opportunity\n",
    "    \n",
    "    # Normalize returns (higher is better) - handle NaN and infinite values\n",
    "    returns_clean = ml_df['annual_return'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    if returns_clean.max() != returns_clean.min():\n",
    "        returns_score = (returns_clean - returns_clean.min()) / (returns_clean.max() - returns_clean.min())\n",
    "    else:\n",
    "        returns_score = pd.Series([0.5] * len(ml_df))\n",
    "    \n",
    "    # Normalize volatility (lower is better, so invert)\n",
    "    volatility_clean = ml_df['volatility'].replace([np.inf, -np.inf], np.nan).fillna(ml_df['volatility'].median())\n",
    "    if volatility_clean.max() != volatility_clean.min():\n",
    "        volatility_score = 1 - ((volatility_clean - volatility_clean.min()) / (volatility_clean.max() - volatility_clean.min()))\n",
    "    else:\n",
    "        volatility_score = pd.Series([0.5] * len(ml_df))\n",
    "    \n",
    "    # PE ratio score (lower is generally better)\n",
    "    pe_clean = ml_df['pe_ratio'].replace([np.inf, -np.inf], np.nan).fillna(ml_df['pe_ratio'].median())\n",
    "    if pe_clean.max() != pe_clean.min():\n",
    "        pe_score = 1 - ((pe_clean - pe_clean.min()) / (pe_clean.max() - pe_clean.min()))\n",
    "    else:\n",
    "        pe_score = pd.Series([0.5] * len(ml_df))\n",
    "    \n",
    "    # ROE score (higher is better)\n",
    "    if 'roe' in ml_df.columns and ml_df['roe'].notna().sum() > 0:\n",
    "        roe_clean = ml_df['roe'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        if roe_clean.max() != roe_clean.min() and roe_clean.max() > 0:\n",
    "            roe_score = (roe_clean - roe_clean.min()) / (roe_clean.max() - roe_clean.min())\n",
    "        else:\n",
    "            roe_score = pd.Series([0.5] * len(ml_df))\n",
    "    else:\n",
    "        roe_score = pd.Series([0.5] * len(ml_df))  # neutral if no data\n",
    "    \n",
    "    # Combine scores (you can adjust weights)\n",
    "    ml_df['investment_score'] = (\n",
    "        0.3 * returns_score + \n",
    "        0.25 * volatility_score + \n",
    "        0.25 * pe_score + \n",
    "        0.2 * roe_score\n",
    "    )\n",
    "    \n",
    "    # Remove any remaining NaN or infinite values in target\n",
    "    ml_df['investment_score'] = ml_df['investment_score'].replace([np.inf, -np.inf], np.nan)\n",
    "    ml_df = ml_df.dropna(subset=['investment_score'])\n",
    "    \n",
    "    print(f\"🧹 After cleaning: {len(ml_df)} stocks with valid scores\")\n",
    "    \n",
    "    # Select features for ML model\n",
    "    feature_cols = ['pe_ratio', 'market_cap', 'annual_return', 'volatility', \n",
    "                   'price_to_high_ratio', 'dividend_yield']\n",
    "    \n",
    "    # Add optional features if available and have enough data\n",
    "    optional_features = ['debt_to_equity', 'roe', 'profit_margin', 'beta']\n",
    "    for feat in optional_features:\n",
    "        if feat in ml_df.columns and ml_df[feat].notna().sum() > len(ml_df) * 0.3:\n",
    "            feature_cols.append(feat)\n",
    "    \n",
    "    # Final cleaning of features\n",
    "    X = ml_df[feature_cols].copy()\n",
    "    for col in feature_cols:\n",
    "        X[col] = X[col].replace([np.inf, -np.inf], np.nan).fillna(X[col].median())\n",
    "    \n",
    "    y = ml_df['investment_score']\n",
    "    \n",
    "    # Final check for any remaining issues\n",
    "    if X.isna().any().any():\n",
    "        print(\"⚠️ Still have NaN in features, filling with 0\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    if y.isna().any():\n",
    "        print(\"⚠️ Still have NaN in target, removing these rows\")\n",
    "        valid_idx = ~y.isna()\n",
    "        X = X[valid_idx]\n",
    "        y = y[valid_idx]\n",
    "        ml_df = ml_df[valid_idx]\n",
    "    \n",
    "    print(f\"📊 Final dataset: {len(X)} stocks with {len(feature_cols)} features\")\n",
    "    print(\"Features:\", feature_cols)\n",
    "    print(f\"🎯 Target variable: investment_score (range: {y.min():.3f} - {y.max():.3f})\")\n",
    "    \n",
    "    return X, y, ml_df, feature_cols\n",
    "\n",
    "# Run feature engineering\n",
    "X, y, ml_df, feature_cols = create_features_and_target(stock_df)\n",
    "print(f\"\\n✅ Feature engineering complete!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Training Random Forest model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[1;32m     15\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     16\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     17\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     18\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 Training Random Forest model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m train_score \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mscore(X_train_scaled, y_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    364\u001b[0m     X,\n\u001b[1;32m    365\u001b[0m     y,\n\u001b[1;32m    366\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    368\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[1;32m    369\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1302\u001b[0m     X,\n\u001b[1;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1328\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[0;32m-> 1328\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1329\u001b[0m         y,\n\u001b[1;32m   1330\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1331\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1332\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1333\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1334\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1335\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m   1336\u001b[0m     )\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Train ML Model\n",
    "# ============================================================================\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"🤖 Training Random Forest model...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "train_score = rf_model.score(X_train_scaled, y_train)\n",
    "test_score = rf_model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"✅ Model trained successfully!\")\n",
    "print(f\"📈 Training R² Score: {train_score:.3f}\")\n",
    "print(f\"🎯 Test R² Score: {test_score:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🔍 Most Important Features:\")\n",
    "for idx, row in feature_importance.head().iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
